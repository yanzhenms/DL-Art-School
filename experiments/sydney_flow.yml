# Notes:
#### general settings
name: sydney_flow #CHANGEME_your_experiment_name
model: extensibletrainer
distortion: sr  # TODO: ????????
scale: 1
gpu_ids: [3] # <-- unless you have multiple gpus, use this
start_step: 0 # -1 causes 0.pth to be saved!
#force_start_step: 39000
checkpointing_enabled: true  # <-- Gradient checkpointing. Enable for huge GPU memory savings. Disable for distributed training.
grad_scaler_enabled: false # TODO: ????????
fp16: false # TODO: why does enabling this with 8bit slow down perf??
use_8bit: true
use_tb_logger: true
wandb: false  # <-- enable to log to wandb. tensorboard logging is always enabled.

# diffusion doesn't actually need paired voice labels...
# HOWEVER, all of the other data fetching modes don't actually provide 'wav' outputs in the dict.
# Possibly could be made more efficient in the future.
datasets:
  train:
    # Data path
    mode: torchtts
    raw_data: /data/yanzhen/LibriTTS
    data_dir: /data/chong/sydney #/data/stdstoragetts01scus_ipgsp/ruiqingxue/data/enus2/goodspeaker_source0831_refine/train/v2_sydney_e_s #/data/chong/libriTTS_data_small #/data/stdstoragetts01scus_datablob/realisticttsdataset_v2.1/Training
    shard_format: tar
    shard_masks: en-us_EnUSSydney_*.tar #en-us_libriTTSR_realtext_0000*_general-00000.tar #en-us_libriTTSR_*.tar
    shard_name: shards
    split: 'train'  # train or train-dev
    # Data parameters
    shard_size: 2000
    # Dataloader parameters
    n_workers: 2
    pin_memory: True
    dynamic_batch: False
    # Batch size in frame
    batch_size: 16
    num_samples: 290000
    vocab_path: ../experiments/bpe_lowercase_asr_256.json #../experiments/bpe_lowercase_asr_256.json
    with_stat_data: True
    with_text_data: True
    with_context_info: True
    sample_rate: 22050
    lazy_decode: True
    conditioning_length: 102400 # WAS 44000
    max_text_tokens: 400
    max_audio_length: 441000 # WAS: 255995 

    # name: TrainDS
    # n_workers: 4 # idk what this does
    # batch_size: 32 # gives ~16GB on my 3090
    # mode: paired_voice_audio
    # sample_rate: 22050
    # path: /home/chong/data/LJSpeech-1.1-large-train/metadata.csv #CHANGEME_training_dataset_name
    # fetcher_mode: ['lj'] # CHANGEME if your dataset isn't in LJSpeech format
    # max_wav_length: 220500 # WAS: 255995 
    # max_text_length: 999 # WAS: 200
    # load_conditioning: true # note that the diffuser doesn't use this, but the gpt model that passes outputs to the diffuser does.
    # num_conditioning_candidates: 1 # WAS: 2
    # conditioning_length: 102400 # WAS 44000
    # needs_collate: false # TODO: ??????????
    # use_bpe_tokenizer: true
    # load_aligned_codes: false
    # phase: train

  # val:
  #   name: TestDS
  #   n_workers: 2
  #   batch_size: 32 # this could be higher probably
  #   mode: paired_voice_audio
  #   sample_rate: 22050
  #   path: /home/chong/data/LJSpeech-1.1-large-val/metadata.csv #CHANGEME_validation_dataset_name
  #   fetcher_mode: ['lj']
  #   max_wav_length: 220500 # WAS: 255995 
  #   max_text_length: 999 # WAS: 200
  #   load_conditioning: true
  #   num_conditioning_candidates: 1 # WAS 2
  #   conditioning_length: 102400 # WAS 44000
  #   needs_collate: false # TODO: ??????????
  #   use_bpe_tokenizer: true
  #   load_aligned_codes: false
  #   phase: val


steps:
  ddpm_train:
    training: ddpm
    loss_log_buffer: 2000 #????
    step_outputs: [loss]

    optimizer: adamw
    #optimizer: lion
    optimizer_params:
      lr: !!float 2e-5 # TODO: change this to something more reasonable
      #lr: !!float 2e-6 # USE LOWER LR for LION
      triton: false # ONLY RELEVANT FOR LION
      weight_decay: 0.001
      beta1: 0.9
      beta2: 0.999 # ??? CHANGEME this is 0.98 elsewhere
    clip_grad_eps: 1.0 # ??? CHANGEME this is 5.0 elsewhere

    injectors:
      to_mel:
        type: torch_mel_spectrogram
        mel_norm_file: ../experiments/clips_mel_norms.pth
        in: wav
        out: mel
      resample_wav:
        type: audio_resample
        in: wav
        out: wav_for_vocoder
        input_sample_rate: 22050
        output_sample_rate: 24000
      tacotron_mel:
        type: mel_spectrogram
        mel_fmax: 12000
        sampling_rate: 24000
        n_mel_channels: 100
        # Only normalize the MEL target, because the diffuser specifically cares about it.
        do_normalization: true
        in: wav_for_vocoder
        out: target_mel
      resample_cond:
        type: for_each
        subtype: audio_resample
        input_sample_rate: 22050
        output_sample_rate: 24000
        in: conditioning
        out: conditioning_for_vocoder
      cond_to_mel:
        type: for_each
        subtype: mel_spectrogram
        mel_fmax: 12000
        sampling_rate: 24000
        n_mel_channels: 100
        in: conditioning_for_vocoder
        out: cond_mel
      produce_latents:
        type: gpt_voice_latent
        gpt_path: /data/chong/tortoise_models/50000_gpt.pth #../experiments/autoregressive.pth #CHANGEME_path_to_finetuned_gpt_checkpoint #../experiments/finetune_gpt_unified_large_kennedy/models/800_gpt_ema.pth
        in: wav
        conditioning_clip: conditioning
        text: text
        text_lengths: text_lengths
        input_lengths: wav_lengths
        out: gpt_latent
      diffusion:
        type: gaussian_diffusion
        in: target_mel
        generator: ddpm
        beta_schedule:
          schedule_name: linear
          num_diffusion_timesteps: 4000 # :(
        diffusion_args:
          model_mean_type: epsilon
          model_var_type: learned_range
          loss_type: mse
        sampler_type: uniform
        model_input_keys:
          aligned_conditioning: gpt_latent
          conditioning_input: cond_mel
          return_code_pred: true
        extra_model_output_keys: [mel_pred]
        out: loss
        out_key_vb_loss: vb_loss
        out_key_x_start: x_start_pred
    losses:
      diffusion_loss:
        after: 200 # TODO: figure out what the right number for this is
        type: direct
        weight: 1
        key: loss
      var_loss:
        after: 200
        type: direct
        weight: 1
        key: vb_loss
      mel_surrogate: # TODO: figure out if this is useful at all
        type: pix
        weight: 1
        criterion: l2
        real: target_mel
        fake: mel_pred

networks:
  ddpm:
    type: generator 
    which_model_G: diffusion_tts_flat # https://github.com/neonbjb/DL-Art-School/issues/5#issuecomment-1142280204
    kwargs:
      model_channels: 1024
      num_layers: 10
      in_channels: 100
      out_channels: 200
      in_latent_channels: 1024
      in_tokens: 8193
      dropout: 0 # could be 0.1?
      use_fp16: false
      num_heads: 16
      layer_drop: 0 # not even 0.1 in default
      unconditioned_percentage: 0.1 # CHANGEME might be better to be 0!
     #freeze_everything_except_autoregressive_inputs: False # probably never a good thing???

path:
  # pretrain_model_gpt: '~/.cache/tortoise/models/diffusion_decoder_ori.pth'  # '~/DL-Art-School/experiments/LJ_diff_5/models/11520_ddpm.pth'# CHANGEME: copy this from tortoise cache
  strict_load: true
  resume_state: ../experiments/sydney_flow/training_state/49280.state # <-- Set this to resume from a previous training state.

# CHANGEME: ALL OF THESE PARAMETERS SHOULD BE EXPERIMENTED WITH
# afaik all units here are measured in **steps** (i.e. one batch of batch_size is 1 unit)
train:
  niter: 64000
  warmup_iter: -1 # could be 0???
  warmup_steps: 0 # TODO ????????
  mega_batch_factor: 8    # <-- Gradient accumulation factor. If you are running OOM, increase this to [2,4,8].
  val_freq: 160 # Personally I believe this should be set to epoch size * 3
  sort_key: wav_lengths # TODO: ????
  auto_collate: true # TODO: ????

  default_lr_scheme: MultiStepLR
  gen_lr_steps: [ 6400, 12800, 25600,38400] # TODO ??? [500, 1000, 1400, 1800] #[50000, 100000, 140000, 180000]
  lr_gamma: 0.5 # TODO ??? 0.5
  ema_enabled: false
  #ema_rate: .9995
  #ema_on_cpu: true
  #manual_seed: 1337 # add this if you want reproducibility

# eval:
#   pure: true # see train.py
# evaluators: TODO
#   fid:
#     for: generator
#     type: audio_diffusion_fid
#     eval_tsv: /y/libritts/test-clean/transcribed-brief-w2v.tsv
#     diffusion_steps: 50
#     diffusion_schedule: linear
#     diffusion_type: tts9_mel_autoin
#     conditioning_free: false
#     conditioning_free_k: 1

logger:
  print_freq: 32 # TODO: set this to epoch size
  save_checkpoint_freq: 1280 # Personally I think this should be the same as val_freq
  visuals: [gen, mel] # idk
  visual_debug_rate: 99999999 # still not using this yet
  is_mel_spectrogram: true
  disable_state_saving: false # CHANGEME if you plan to halt training inbetween

upgrades:
  # Variable: number_of_checkpoints_to_save
  # Description: Define how many checkpoints should be saved on disk (1 checkpoint = pth+ =~ 6.8 GB)
  # Type: integer
  # Value: should be the same value as for number_of_states_to_save
  # smaller than 1 - turn off this option; there is no max value. For Colab use 1 or 2.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved checkpoint + last saved state (about 6.8 GB).
  # 2 == Leave last 2 saved checkpoints + last saved states (about 2 *~ 6.8 GB =~ 13.6 GB).
  number_of_checkpoints_to_save: -1
  # Variable: number_of_states_to_save
  # Description: Define how many states should be saved on disk (1 state =~ 3.4 GB)
  # if disable_state_saving is set as true this option will be inactive
  # Type: integer
  # Value: should be the same value as for number_of_checkpoints_to_save
  # smaller than 1 - turn off this option; there is no max value.
  # For Colab use 1 or 2 for gDrive and 5 for instance drive
  # 1 == Leave last saved state (about 3.4 GB).
  # 2 == Leave last 2 saved states (about 2 *~ 3.4 GB =~ 6.8 GB).
  number_of_states_to_save: -1
